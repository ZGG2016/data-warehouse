a2.sources=r1 r2
a2.channels=c1 c2
a2.sinks=k1 k2

## source1
a2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r1.batchSize = 5000
a2.sources.r1.batchDurationMillis = 2000
a2.sources.r1.kafka.bootstrap.servers = bigdata101:9092
a2.sources.r1.kafka.topics=topic_start

## source2
a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r2.batchSize = 5000
a2.sources.r2.batchDurationMillis = 2000
a2.sources.r2.kafka.bootstrap.servers = bigdata101:9092
a2.sources.r2.kafka.topics=topic_event

# channel1
a2.channels.c1.type = file
a2.channels.c1.checkpointDir = /opt/flume-1.7.0/checkpoint/behavior1
a2.channels.c1.dataDirs = /opt/flume-1.7.0/data/behavior1/
a2.channels.c1.keep-alive = 6

# channel2
a2.channels.c2.type = file
a2.channels.c2.checkpointDir = /opt/flume-1.7.0/checkpoint/behavior2
a2.channels.c2.dataDirs = /opt/flume-1.7.0/data/behavior2/
a2.channels.c2.keep-alive = 6

# sink1
a2.sinks.k1.type = hdfs
a2.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a2.sinks.k1.hdfs.filePrefix = logstart-

#sink2
a2.sinks.k2.type = hdfs
a2.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a2.sinks.k2.hdfs.filePrefix = logevent-

# 不要产生大量小文件,生产环境 rollInterval 配置为 3600
a2.sinks.k1.hdfs.rollInterval = 10
a2.sinks.k1.hdfs.rollSize = 134217728
a2.sinks.k1.hdfs.rollCount = 0
a2.sinks.k1.hdfs.minBlockReplicas=1 
a2.sinks.k2.hdfs.rollInterval = 10
a2.sinks.k2.hdfs.rollSize = 134217728
a2.sinks.k2.hdfs.rollCount = 0
a2.sinks.k2.hdfs.minBlockReplicas=1

# 控制输出文件是原生文件。
a2.sinks.k1.hdfs.fileType = CompressedStream
a2.sinks.k2.hdfs.fileType = CompressedStream
a2.sinks.k1.hdfs.codeC = lzop
a2.sinks.k2.hdfs.codeC = lzop

# 拼装
a2.sources.r1.channels = c1
a2.sinks.k1.channel= c1
a2.sources.r2.channels = c2
a2.sinks.k2.channel= c2
